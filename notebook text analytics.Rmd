---
title: "R Notebook"
output:
  html_notebook:
    toc: yes
---
```{r loading libs and data, message=FALSE, warning=FALSE, include=FALSE}
#lecture 1
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tokenizers)
library(tidytext)
library(SnowballC)
library(tm)
library(stringi)
library(ggrepel)
library(wordcloud)
library(syuzhet)
library(qdap)
library(readr)
#lecture 2
library(smacof)
library(ggfortify)
library(ggthemes)
library(stats)
library(quanteda)
#mds pcs
library(factoextra)
#lecture 3
library(syuzhet)
# maybe needed
# options(stringsAsFactors = FALSE)

# wd Ertiza
# wd Shane
# wd <- "C:/Users/mingh/Documents/Text-analytics"
# wd Najla <- 
# setwd(wd)
reviews_df <- read.csv("Womens Clothing E-Commerce Reviews.csv")
```

# Cleaning and stemming

```{r merge title and review body, include=FALSE}
reviews_df$text_full <- paste(reviews_df$Title, reviews_df$Review.Text, sep = " ")
```

I remove row without text here.
```{r remove rows without text ,include=FALSE}
reviews_df <- reviews_df[!(reviews_df$text_full == " "), ]
```

I add rating variable in binairy here
```{r}
reviews_df$Rating_b <- "happy"
reviews_df[reviews_df$Rating < 4,]$Rating_b <- "unhappy"
```


```{r Remove unnecessary punctuation}
reviews_df$Review <- as.character(reviews_df$text_full)  %>% 
  tolower() %>% 
  {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
  {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
  {gsub("(\"| |\\$)-+\\.-+"," NUMBER", .)} %>%     # Find numbers
  {gsub("([0-9]+:)*[0-9]+ *am"," TIME_AM", .)} %>%         # Find time AM
  {gsub("([0-9]+:)*[0-9]+ *pm"," TIME_PM", .)} %>%         # Find time PM
  {gsub("-+:-+","TIME", .)} %>%                    # Find general time
  {gsub("\\$ ?[0-9]*[\\.,]*[0-9]+"," DOLLARVALUE ", .)} %>%           # Find Dollar values
  {gsub("[0-9]*[\\.,]*[0-9]+"," NUMBER ", .)} %>%           # Find remaining numbers
  {gsub("-"," ", .)} %>%                           # Remove all -
  {gsub("&"," and ", .)} %>%                       # Find general time
  {gsub("\"+"," ", .)} %>%                         # Remove all "
  {gsub("\\|+"," ", .)} %>%                        # Remove all |
  {gsub("_+"," ", .)} %>%                          # Remove all _
  {gsub(";+"," ", .)} %>%                          # Remove excess ;
  {gsub(" +"," ", .)} %>%                          # Remove excess spaces
  {gsub("\\.+","\\.", .)} %>%                      # Remove excess .
  {gsub("\'\"+"," ", .)}                        # Remove all '
```

```{r Remove stop words + barplot freq words, echo=FALSE}
#cut text into words by splitting on spaces and punctuation
review_words <- reviews_df %>% unnest_tokens(word,Review,to_lower=TRUE) 
review_words <- review_words %>% 
                        anti_join(stop_words)
counts <- review_words_nostop %>%
            count(word, sort=TRUE)
counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(30, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram stop word removed")
```

```{r stemming, message=FALSE, warning=FALSE, include=FALSE}
# Creating the full review from the cleaned+stemmedwords

for (j in 1:nrow(reviews_df)) {
  stemmed_Review<-  anti_join((reviews_df[j,] %>% unnest_tokens(word,Review, drop=FALSE,to_lower=TRUE) ),stop_words)
   
  stemmed_Review<-(wordStem(stemmed_Review[,"word"], language = "porter"))
 
  reviews_df[j,"Review_stem"]<-paste((stemmed_Review),collapse = " ")
   
 }
```

```{r plot stemmed words, echo=FALSE, message=FALSE, warning=FALSE}
#cut text into words by splitting on spaces and punctuation
review_words_stem <- reviews_df %>% unnest_tokens(word,Review_stem,to_lower=TRUE) 
counts <- review_words_stem %>%count(word, sort=TRUE) 

counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(30, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram after stop and stem")

```

# plotting simple vis

```{r wordcloud}
wordcloud(words = counts$word , freq = counts$n, min.freq = 1000,
          max.words=70, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

```{r dataframes for different reviews, include=FALSE}
wordcount_happy <- reviews_df %>% unnest_tokens(word,Review_stem) %>% filter(Rating > 3) %>%count(word, sort=TRUE)
names(wordcount_happy)[2] <- "n_happy"

wordcount_unhappy <- reviews_df %>% unnest_tokens(word,Review_stem) %>% filter(Rating < 2) %>%count(word, sort=TRUE)
names(wordcount_unhappy)[2] <- "n_unhappy"
```
## happy 
```{r plot happy}
wordcount_happy %>% 
  mutate(word = reorder(word,n_happy)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n_happy)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram Happy")

wordcloud(words = wordcount_happy$word , freq = wordcount_happy$n, min.freq = 1000,
          max.words=70, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
## unhappy
```{r plot unhappy}
wordcount_unhappy %>% 
  mutate(word = reorder(word,n_unhappy)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n_unhappy)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram unHappy")


wordcloud(words = wordcount_unhappy$word , freq = wordcount_unhappy$n, min.freq = 1000,
          max.words=70, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))



```

**switching to document term matrix (this is the dfm (document frequency) format in quanteda )**
word freq total
```{r corpus setup, echo=FALSE}
corpus <- corpus(reviews_df, docid_field = "X", text_field = "Review_stem",  metacorpus = NULL, compress = TRUE)
review.dfm <- dfm(corpus)
wordfreqs <- colSums(as.matrix(review.dfm)) 
wordfreqs <- data.frame(word = names(wordfreqs), n=wordfreqs)
wordfreqs %>%
  mutate(word = reorder(word,n)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram corpus")
```

Going over to document frequency (How many documents contain these words pretty sure)
```{r document frequency,echo=FALSE}
docfreqs <- docfreq(review.dfm) %>% sort(decreasing = TRUE)
docfreqs <- data.frame(word = names(docfreqs), n_docs=docfreqs)
docfreqs %>%
  mutate(word = reorder(word,n_docs)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n_docs)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Document Frequency Histogram ")
```

```{r ratio dfs, include=FALSE}
tf_idf_table <- merge(docfreqs, wordfreqs)

tf_idf_table$tf_idf <- tf_idf_table$n/tf_idf_table$n_docs

tf_idf_table<-tf_idf_table[order(-tf_idf_table$tf_idf),]
```

I think it's these words because it's a ratio. Should we divide termfreq/docfreq or  docfreq/termfreq.
```{r TF-IDF plot,echo=FALSE}
tf_idf_table %>%
  mutate(word = reorder(word,tf_idf)) %>% 
  top_n(7, tf_idf) %>%
  ggplot(aes(word,tf_idf)) +  
  geom_col() + 
  labs(x = NULL, y = "tf_idf") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("TF-IDF value ")
```

Plot happy unhappy ratios
```{r ,include=FALSE}
both_un_happy <- merge(wordcount_unhappy, wordcount_happy)

both_un_happy$ratio <- both_un_happy$n_happy / both_un_happy$n_unhappy

both_un_happy<-both_un_happy[order(-both_un_happy$ratio),]
```

```{r plot unhappy happy ratio, echo=FALSE}
both_un_happy %>%
  mutate(word = reorder(word,ratio)) %>% 
  top_n(10, ratio) %>%
  ggplot(aes(word,ratio)) +  
  geom_col() + 
  labs(x = NULL, y = "ratio") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Relatively most frequent words in happy ")

both_un_happy %>%
  mutate(word = reorder(word,-ratio)) %>% 
  top_n(-10, ratio) %>%
  ggplot(aes(word,ratio)) +  
  geom_col() + 
  labs(x = NULL, y = "ratio") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Relatively most frequent words in unhappy ")
```

```{r setup and comparisionclouds,echo=FALSE}
reviews_df$happy <- "happy"
reviews_df[reviews_df$Rating <3,]$happy <- "unhappy"
compare_matrix_binary <- reviews_df %>% unnest_tokens(word,Review_stem) %>%count(word,happy,sort=TRUE) %>%ungroup()%>%cast_tdm(word,happy,n)
compare_matrix_rating <- reviews_df %>% unnest_tokens(word,Review_stem) %>%count(word,Rating,sort=TRUE) %>%ungroup()%>%cast_tdm(word,Rating,n)

comparison.cloud(as.matrix(compare_matrix_binary), scale=c(3,0.5), random.order=FALSE, colors = c("indianred3","lightsteelblue3"),
 max.words=30, rot.per = 0.3)

comparison.cloud(as.matrix(compare_matrix_rating), scale=c(3,0.5), random.order=FALSE, colors = c("lightblue", "pink", "orange", "lightgreen", "grey"), 
 max.words=30, rot.per = 0.3)
```

## commonality cloud

So these two plots are similar but that makes sense since, it's about words that are used by all ratings commenly.
```{r rcommonality cloud, echo= false}
commonality.cloud(as.matrix(compare_matrix_binary), scale=c(3,0.5), random.order=FALSE, colors=brewer.pal(8, "Dark2"),
                  max.words=30, rot.per = 0.3)
commonality.cloud(as.matrix(compare_matrix_rating), scale=c(3,0.5), random.order=FALSE, colors=brewer.pal(8, "Dark2"),
                  max.words=30, rot.per = 0.3)
```
# MDS
```{r filter infrequent words, message=FALSE, warning=FALSE, include=FALSE}
#can use this maybe I think this is correct?
infrequent <- counts %>% filter(n<0.01*nrow(counts))
frequent <- counts %>% filter(n>0.01*nrow(counts))
toremove <- infrequent

for (j in 1:nrow(reviews_df)) {
 stemmed_Review<-  anti_join((reviews_df[j,] %>% unnest_tokens(word,Review_stem,to_lower=TRUE) ),toremove)
  
 reviews_df[j,"Review_after_infreq"]<-   paste((stemmed_Review[,"word"]),collapse = " ")

}
```
## MDS document
```{r MDS document,echo=FALSE, message=FALSE, warning=FALSE}
reviews_corp <- corpus(reviews_df, docid_field = "X", text_field = "Review_after_infreq")

# feature cooccurrence matrix : fcm()
co_occurrence_matrix <- fcm(x = reviews_corp, context = "document", count = "frequency", tri=FALSE)

reviews_dfm <- dfm(reviews_corp) # get document frequency matrix
counts_dfm <- colSums(as.matrix(reviews_dfm)) 
co_occurrence_matrix <- as.matrix(co_occurrence_matrix)
diag(co_occurrence_matrix) <- counts_dfm

sortedcount <- counts_dfm%>% sort(decreasing=TRUE)
sortednames <- names(sortedcount)
nwords<-200
subset_words<-as.matrix(sortedcount[1:nwords])
co_occurrence_matrix <- co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]
co_occurrence_matrix[1:15,1:15]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D mp given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```
## MDS window = 1
```{r MDS window is  1, echo=FALSE}
co_occurrence_matrix <- fcm(x = reviews_corp, context = "window", window=1, count = "frequency", tri=FALSE)

co_occurrence_matrix<-co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]

diag(co_occurrence_matrix) <- counts_dfm[sortednames[1:nwords]]
co_occurrence_matrix[1:15,1:15]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
min(distances) #check whethet minimum distance is positive. Sometimes the counting procedure did something unexpected.
max(distances) #check whethet minimum distance is positive. Sometimes the counting procedure did something unexpected.
MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D mp given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```
## MDS window = 5
```{r MDS window is  5, echo=FALSE}
co_occurrence_matrix <- fcm(x = reviews_corp, context = "window", window=5, count = "boolean", tri=FALSE)

co_occurrence_matrix<-co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]

diag(co_occurrence_matrix) <- counts_dfm[sortednames[1:nwords]]
co_occurrence_matrix[1:15,1:15]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D map given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```

```{r PCA, echo=FALSE, message=FALSE, warning=FALSE}
review_tdm <- reviews_df %>% unnest_tokens(word,Review_after_infreq) %>%count(word,X,sort=TRUE) %>%ungroup()%>%cast_tdm(word,X,n)

counts <- rowSums(as.matrix(review_tdm)) 
sortedcount <- counts%>% sort(decreasing=TRUE)
nwords<-200
sortednames <- names(sortedcount[1:nwords])
pca_results <- prcomp(t(review_tdm), scale = FALSE, rank. = 50) # why do we set scale to FALSE? -> All data is measured on the same scale 
pca_results_backup <- pca_results  # create a backup of results for later use
fviz_screeplot(pca_results,ncp=50)

```

```{r}
ncomp<-8
pca_results_backup -> pca_results #restore original to play with number of components and rotations
```
Top words per component
```{r top words per component}
#select the most important words per dimension

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords

```

```{r warning = FALSE}
axeslist <- c(3, 4)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE     # Avoid text overlapping
             )

```
Still need vis for PCA and also might need to look at this a bit more.

# sentiment analysis

## plot freqeuent positive and negative words
```{r plot freqeuent positive and negative words}
# Remove stop words and count frequencies (only keep those with frequency above 50)
all_words <- reviews_df[,] %>%
             unnest_tokens("Review", output = "word") %>%
             anti_join(stop_words, by = "word") %>%
             count(word, sort = TRUE) %>%
             filter(n>50)

# get sentiment with bing dictionary:
# all_words$sentiment <- get_sentiment(all_words$word, method = "bing", language = "english") 

sentiment_scores <- polarity(all_words$word)$all        # get sentiment with polarity function
all_words$sentiment <- sentiment_scores[,"polarity"]

# Create plot
all_words %>%
  filter(n > 250) %>%
  filter(sentiment != 0) %>%
  mutate(n = ifelse(sentiment == -1, -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  mutate(Sentiment = ifelse(sentiment == 1, "Postive","Negative")) %>%
  ggplot(aes(word, n, fill = Sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to \"total\" sentiment", x = "Word (min freq = 50)")
```

```{r create sentiment per review different methods, echo=FALSE}
pol <- polarity(reviews_df[, "Review"])$all
reviews_df$polarity <- pol[, "polarity"]
reviews_df$sent_bing  <- get_sentiment(reviews_df$Review, method = "bing")

# Some alternatives
#reviews_df$sent_syu  <- get_sentiment(reviews_df$Description, method = "syuzhet")
#reviews_df$sent_afinn<- get_sentiment(reviews_df$Description, method = "afinn")
#reviews_df$sent_nrc  <- get_sentiment(reviews_df$Description, method = "nrc")

```


```{r polarity and bing of pos and neg, echo=FALSE}
pos_reviews      <- reviews_df %>% filter(Rating > 3)
neg_reviews      <- reviews_df %>% filter(Rating < 4)

for (v in c("sent_bing","polarity"))
{
  cat(v)
  cat(" ", mean(neg_reviews[, v]), " ", mean(pos_reviews[, v]), "\n")
}
```

## plot polarity and bing
```{r plot polarity and bing, echo=FALSE}

ggplot(reviews_df ,aes(sent_bing)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Evaluation", values = c("green", "red"), aesthetics = "fill")

ggplot(reviews_df,aes(sent_bing)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Evaluation", values = c("green", "red"), aesthetics = "fill")

ggplot(reviews_df ,aes(polarity)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Evaluation", values = c("green", "red"), aesthetics = "fill")

ggplot(reviews_df,aes(polarity)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Evaluation", values = c("green", "red"), aesthetics = "fill")


```

## plot polarity pos/neg words
```{r Word frequencies in neg/pos reviews using polarity}
# select negative reviews
all_neg_review_words <- reviews_df  %>% filter(reviews_df$polarity<0)

# get corresponding words (remove stop words)
all_neg_review_words <- all_neg_review_words %>%
                        unnest_tokens(word, Review) %>%
                        anti_join(stop_words, by = "word")

# create plot
all_neg_review_words%>%
            count(word, sort=TRUE) %>%
            mutate(word = reorder(word,n)) %>%
            top_n(25, word) %>%
            ggplot(aes(word,n)) +  
            geom_col() +
            labs(x = NULL, y = "Number of occurences") +
            coord_flip() +
            theme(text = element_text(size = 17)) +
            ggtitle("Word Frequencies: negatively classified reviews")

all_pos_review_words <- reviews_df  %>% filter(reviews_df$polarity>0)

all_pos_review_words <- (all_pos_review_words) %>% 
                        unnest_tokens(word, Review) %>% 
                        anti_join(stop_words, by = "word")

all_pos_review_words%>%
            count(word, sort=TRUE) %>%
            mutate(word = reorder(word,n)) %>%
            top_n(25, word) %>%
            ggplot(aes(word,n)) +  
            geom_col() +
            labs(x = NULL, y = "Number of occurences") +
            coord_flip() +
            theme(text = element_text(size = 17)) +
            ggtitle("Word Frequencies: positively classified reviews")

```

## sentiment analysis: number of positive sentences - number of negative sentences

(this code generates some warnings on 'double punctuation')
```{r, warning=FALSE}
# This needs to be done by review as it decomposes into sentences

# Allocate space for new variables
reviews_df <- cbind(reviews_df, sentence_sent_polarity = 0*cbind(1:nrow(reviews_df)),
                                sentence_sent_syu      = 0*cbind(1:nrow(reviews_df)),
                                sentence_sent_afinn    = 0*cbind(1:nrow(reviews_df)),
                                sentence_sent_bing     = 0*cbind(1:nrow(reviews_df)),
                                sentence_sent_nrc      = 0*cbind(1:nrow(reviews_df)))

totalNoSentences = 0
for (j in 1:nrow(reviews_df)) {
  # print progress
  if (j%%1000==0){
    print(100*j/nrow(reviews_df))
  }
  localSentences <- get_sentences(reviews_df[j,]$Review)
  totalNoSentences = totalNoSentences + length(localSentences)
  
  reviews_df[j,]$sentence_sent_polarity <- polarity(localSentences)$all[,"polarity"] %>% 
                                            sign() %>% sum()
  
  #reviews_df[j,]$sentence_sent_syu <-
  #    get_sentiment(localSentences, method = "syuzhet", language = "english") %>% sign()  %>% sum()

  #reviews_df[j,]$sentence_sent_afinn <-
  #  get_sentiment(localSentences, method = "afinn", language = "english") %>% sign()  %>% sum()

  reviews_df[j,]$sentence_sent_bing <-
    get_sentiment(localSentences, method = "bing", language = "english") %>% sign()  %>% sum()

  #reviews_df[j,]$sentence_sent_nrc <-
  #  get_sentiment(localSentences, method = "nrc", language = "english") %>% sign()  %>% sum()
}
```

```{r}
all_sentences <- data.frame(sentence    = rep(0, totalNoSentences),
                            sentiment   = rep(0, totalNoSentences),
                            polarity    = rep(0, totalNoSentences),
                            User_ID     = rep(reviews_df[1,"X"],     totalNoSentences),
                            Rating = rep(reviews_df[1,"Rating_b"], totalNoSentences), 
                            stringsAsFactors = FALSE)
#totalNoSentences <- 116396 # this was the number after the previous step
sentenceIndex = 1
for (j in 1:nrow(reviews_df)) {
  # show some progress 
  if (j%%2000==0){
    print(100*j/nrow(reviews_df))
  }
  
  localSentences <- reviews_df[j,]$Review %>% get_sentences()
  
  all_sentences[sentenceIndex : (sentenceIndex+length(localSentences)-1), "sentence"] <- localSentences

  all_sentences[sentenceIndex : (sentenceIndex+length(localSentences)-1), "sentiment"] <- get_sentiment(localSentences, method = "bing")
  all_sentences[sentenceIndex : (sentenceIndex+length(localSentences)-1), "polarity"] <- polarity(localSentences)$all[,"polarity"]

  all_sentences[sentenceIndex : (sentenceIndex+length(localSentences)-1), "User_ID"]  <- reviews_df[j,]$X
  all_sentences[sentenceIndex : (sentenceIndex+length(localSentences)-1), "Rating"] <- reviews_df[j,]$Rating_b

  sentenceIndex = sentenceIndex + length(localSentences)
}
```

